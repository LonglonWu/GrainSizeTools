# ============================================================================ #
#                                                                              #
#    GrainSizeTools Script                                                     #
#    A Python script for estimating grain size features from thin sections     #
#                                                                              #
#    Copyright (c) 2014-present   Marco A. Lopez-Sanchez                       #
#                                                                              #
#    Licensed under the Apache License, Version 2.0 (the "License");           #
#    you may not use this file except in compliance with the License.          #
#    You may obtain a copy of the License at                                   #
#                                                                              #
#        http://www.apache.org/licenses/LICENSE-2.0                            #
#                                                                              #
#    Unless required by applicable law or agreed to in writing, software       #
#    distributed under the License is distributed on an "AS IS" BASIS,         #
#    WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.  #
#    See the License for the specific language governing permissions and       #
#    limitations under the License.                                            #
#                                                                              #
#    Version 2.0                                                               #
#    For details see: http://marcoalopez.github.io/GrainSizeTools/             #
#    download at https://github.com/marcoalopez/GrainSizeTools/releases        #
#                                                                              #
#    Requirements:                                                             #
#        Python version 3.5.x or higher                                        #
#        Numpy version 1.11 or higher                                          #
#        Matplotlib version 1.5.3 or higher                                    #
#        Scipy version 0.13 or higher                                          #
#        Pandas version 0.16.x or higher                                       #
#                                                                              #
# ============================================================================ #


import os
import matplotlib.pyplot as plt
import matplotlib as mpl
import numpy as np
from numpy import mean, std, median, sqrt, exp, log, array, tan, arctan, delete
from pandas import read_table, read_csv, read_excel, DataFrame
from scipy.stats import gaussian_kde, iqr, sem, t
from scipy.optimize import curve_fit

# Set the plot style. To see the different styles available in Matplotlib see:
# https://tonysyu.github.io/raw_content/matplotlib-style-gallery/gallery.html
mpl.style.use('ggplot')
mpl.rcParams['font.family'] = 'Verdana'
mpl.rcParams['xtick.labelsize'] = 11.
mpl.rcParams['ytick.labelsize'] = 11.


def extract_areas(file_path='auto', col_name='Area'):
    """ Extract the data corresponding to the areas of grain profiles from
    tabular-like text files generated by the ImageJ or similar applications.
    Files can be in plain text (.txt, .csv) or excel (.xlsx) format

    Parameters
    ----------
    file_path : string, optional
        the file location in quotes
        e.g: 'C:/...yourFileLocation.../nameOfTheFile.csv'
        If 'auto' the function will ask you for the location of the file
        through a file selection dialog.

    col_name : string, optional
        the name of the column that contains the areas of the grain profiles.
        'Area' by default.

    Examples
    --------
    areas = extract_areas()
    areas = extract_areas(col_name='grain_areas')

    Call function
    -------------
    - get_filepath

    Returns
    -------
    An array with the column extracted
    """

    if file_path == 'auto':
        file_path = get_filepath()

    if file_path.endswith('.txt'):
        data_frame = read_table(file_path)
        data_set = array(data_frame[col_name])
    elif file_path.endswith('.csv'):
        data_frame = read_csv(file_path)
        data_set = array(data_frame[col_name])
    elif file_path.endswith('.xlsx'):
        data_frame = read_excel(file_path)
        data_set = array(data_frame[col_name])
    else:
        raise TypeError("Error: The file is not a 'txt' nor 'csv' or 'xlsx' or the file extension was not specified.")

    print(' ')
    print(data_frame.head())
    print('...')
    print(data_frame.tail())
    print(' ')
    print('column extracted =', data_set)
    print('n =', len(data_set))
    print(' ')

    return data_set


def get_filepath():
    """ Get the file path through a file selection dialog"""

    try:  # code for Python 3.x versions
        import tkinter as tk
        from tkinter import filedialog
        root = tk.Tk()
        root.withdraw()
        root.attributes("-topmost", True)
        file_path = filedialog.askopenfilename(initialdir=os.getcwd(),
                                               title="Select file",
                                               filetypes=[('Text files', '*.txt'),
                                                          ('Text files', '*.csv'),
                                                          ('Excel files', '*.xlsx')])
    except ImportError:
        print('This script requires Python 3.5 or higher')

    return file_path


def calc_diameters(areas, correct_diameter=None):
    """ Calculate the diameters from sectional areas using the equivalent circular
    diameter.

    Parameters
    ----------
    areas : array_like
        the sectional areas of the grains

    correct_diameter : None or positive scalar, optional
        add the width of the grain boundaries to correct the diameters. If
        correct_diameter is not declared no correction is considered.

    Returns
    -------
    A numpy array with the equivalent circular diameters
    """

    # calculate diameters via equivalent circular diameter
    diameters = 2 * sqrt(areas / np.pi)

    # diameter correction adding edges (if applicable)
    if correct_diameter is not None:
        diameters += correct_diameter

    return diameters


def find_grain_size(diameters, areas=None, plot='lin', binsize='auto'):
    """ Estimate different 1D measures of grain size from a population
    grain sections and plot the location of these measures along with
    the apparent grain size distribution. Includes the arithmetic mean,
    the RMS mean, the area-weighted mean, the median, and the frequency
    peak grain sizes.

    Parameters
    ----------
    diameters : array_like
        the apparent diameters of the grains

    areas : array_like or None, optional
        the areas of the grain profiles

    plot : string, optional
        the scale/type of the plot and grain size estimation.

        | Types:
        | 'lin' for a linear frequency vs diameter
        | 'log' for a frequency vs logarithmic diameter
        | 'sqrt' for a frequency vs square root diameter
        | 'area' for a area-weighted frequency vs diameter.

    binsize : string or positive scalar, optional
        If 'auto', it defines the plug-in method to calculate the bin size.
        When integer or float, it directly specifies the bin size.
        Default: the 'auto' method.

        | Available plug-in methods:
        | 'auto' (fd if sample_size > 1000 or Sturges otherwise)
        | 'doane' (Doane's rule)
        | 'fd' (Freedman-Diaconis rule)
        | 'rice' (Rice's rule)
        | 'scott' (Scott rule)
        | 'sqrt' (square-root rule)
        | 'sturges' (Sturge's rule)

    Call functions
    --------------
    - calc_freq_grainsize
    - calc_areaweighted_grainsize

    Examples
    --------
    find_grain_size(diameters)
    find_grain_size(diameters, plot='log')
    find_grain_size(diameters, areas, plot='area')
    find_grain_size(diameters, binsize='doane')

    Returns
    -------
    A plot with the distribution of apparent grain sizes and several
    statistical parameters
    """

    # determine the grain size parameters using a number-weighted approach
    if plot == 'lin':
        return calc_freq_grainsize(diameters, binsize, plot='linear')

    elif plot == 'log':
        diameters = log(diameters)
        return calc_freq_grainsize(diameters, binsize, plot='log')

    elif plot == 'sqrt':
        diameters = sqrt(diameters)
        return calc_freq_grainsize(diameters, binsize, plot='sqrt')

    # determine the grain size using the area-weighted approach
    elif plot == 'area':
        if areas is None:
            print('You must enter the areas of the grain sections')
            return None
        else:
            return calc_areaweighted_grainsize(areas, diameters, binsize)

    else:
        raise ValueError("The type of plot has been misspelled, please use 'lin', 'log', 'sqrt' or 'area'")


def Saltykov(diameters, numbins=10, set_limit=None, text_file=None, return_data=False):
    """ Estimate the actual (3D) distribution of grain size from the population of
    apparent diameters measured in a thin section using the Saltykov method.
    (Saltykov 1967; Sahagian and Proussevitch 1998).

    The Saltykov method is optimal to estimate the volume of a particular grain size
    fraction as well as to obtain a qualitative view of the appearance of the actual
    3D grain size population, either in uni- or multimodal populations.

    Parameters
    ----------
    diameters : array_like
        the apparent diameters of the grains

    numbins : positive integer, optional
        the number of bins/classes of the histrogram. If not declared, is set
        to 10 by default

    set_limit : positive scalar or None, optional
        if the user specifies a number, the function will return the volume
        occupied by the grain fraction up to that value.

    text_file : string or None, optional
        if the user specifies a name, the function will return a csv file
        with that name containing the data used to construct the Saltykov
        plot

    return_data : bool, optional
       if True the function will return the position of the midpoints and
       the frequencies (use by other functions).

    Call functions
    --------------
    - unfold_population
    - Saltykov_plot

    References
    ----------
    | Saltykov SA (1967) http://doi.org/10.1007/978-3-642-88260-9_31
    | Sahagian and Proussevitch (1998) https://doi.org/10.1016/S0377-0273(98)00043-2

    Return
    ------
    Statistical data, a plot, and a file with the data (optional)
    """
    
    if isinstance(numbins, int) is False:
        raise ValueError('Numbins must be a positive integer')
    if numbins <= 0:
        raise ValueError('Numbins must be a positive integer')

    # compute the histogram
    freq, bin_edges = np.histogram(diameters, bins=numbins, range=(0., max(diameters)), density=True)
    binsize = bin_edges[1]

    # Create an array with the left edges of the bins and other with the midpoints
    left_edges = delete(bin_edges, -1)
    mid_points = left_edges + binsize / 2

    # Applied the Scheil-Schwartz-Saltykov method to unfold the population of apparent diameters
    freq3D = unfold_population(freq, bin_edges, binsize, mid_points)

    # Calculates the volume-weighted cumulative frequency distribution
    x_vol = binsize * (4 / 3.) * np.pi * (mid_points**3)
    freq_vol = x_vol * freq3D
    cdf = np.cumsum(freq_vol)
    cdf_norm = 100 * (cdf / cdf[-1])

    if set_limit is not None:
        x = mid_points
        y = cdf_norm
        index = np.argmax(mid_points > set_limit)
        angle = arctan((y[index] - y[index - 1]) / (x[index] - x[index - 1]))
        volume = y[index - 1] + tan(angle) * (set_limit - x[index - 1])
        if volume < 100.0:
            print('volume fraction (up to', set_limit, 'microns) =', round(volume, 2), '%')
            print(' ')
        else:
            print('volume fraction (up to', set_limit, 'microns) =', 100, '%')
            print(' ')

    # Save a text file with the midpoints, class frequencies, and cumulative volumes
    if text_file is not None:
        if isinstance(text_file, str) is False:
            print('text_file must be None or a string')
        df = DataFrame({'mid_points': np.around(mid_points, 3),
                        'freqs': np.around(freq3D, 4),
                        'cum_vol': np.around(cdf_norm, 2)})
        df.to_csv(text_file, sep='\t')
        print('The file {}.csv was created' .format(text_file))

    if return_data is True:
        return mid_points, freq3D
    elif return_data is False:
        print(' ')
        print('sample size =', len(diameters))
        print('bin size =', round(binsize, 2))
        return Saltykov_plot(left_edges, freq3D, binsize, mid_points, cdf_norm)
    else:
        raise TypeError('return_data must be set as True or False')


def derive3D(diameters, class_range=(12, 20), initial_guess=False):
    """ Estimates the shape of the actual (3D) distribution of grain size from a
    population of apparent diameters measured in a thin section using the two-step
    method (Lopez-Sanchez and Llana-Funez, 2016).

    The method only works properly for unimodal lognormal-like grain size populations
    and returns the MSD (i.e. shape) and the median (i.e. scale) values, which
    describe the lognormal population of grain sizes at their lineal scale. For
    details see Lopez-Sanchez and Llana-Funez (2016).

    Parameters
    ----------
    diameters : array_like
        the apparent diameters of the grains

    numbins : string or positive integer, optional
        the number of bins/classes of the histrogram. If 'auto' (the default),
        an algorithm will estimate the optimal number of classes

    initial_guess : boolean, optional
        If False, the script will use the default guessing values to fit a
        lognormal distribution. If True, the script will ask the user to define
        their own MSD and median guessing values.

    Call functions
    --------------
    - Saltykov
    - gen_xgrid
    - log_function
    - twostep_plot

    References
    ----------
    | Saltykov SA (1967) http://doi.org/10.1007/978-3-642-88260-9_31
    | Sahagian and Proussevitch (1998) https://doi.org/10.1016/S0377-0273(98)00043-2
    | Lopez-Sanchez and Llana-Funez (2016) https://doi.org/10.1016/j.jsg.2016.10.008

    Returns
    -------
    A plot with an estimate of the actual (3D) grains size distribution and
    several statistical parameters
    """

    if initial_guess is False:
        shape = 1.2
        scale = 30.0
    elif initial_guess is True:
        shape = float(input('Define an initial guess for the MSD parameter (the default value is 1.2; MSD > 1.0): '))
        scale = float(input('Define an initial guess for the median parameter (the default value is 30.0): '))
    else:
        raise TypeError('Initial_guess must be set as True or False')

    # estimate the number of classes that produces the best fit
    mini, maxi = class_range
    class_list = list(range(mini, maxi + 1))
    stds = np.zeros(len(class_list))

    for index, item in enumerate(class_list):
        mid_points, frequencies = Saltykov(diameters, numbins=item, return_data=True)
        optimal_params, sigma_error = two_step(mid_points, frequencies, initial_guess=(shape, scale))
        stds[index] = sigma_error[0]

    optimal_num_classes = class_list[np.argmin(stds)]
    mid_points, frequencies = Saltykov(diameters, numbins=optimal_num_classes, return_data=True)
    optimal_params, sigma_err = two_step(mid_points, frequencies, (shape, scale))
    
    print(' ')
    print('Optimal coefficients:')
    print('Number of clasess: {}' .format(optimal_num_classes))
    print('MSD (shape) = {msd} ± {err}' .format(msd=round(optimal_params[0], 2), err=round(3 * sigma_err[0], 2)))
    print('Median (location) = {median} ± {err} (caution: not realiable)' .format(median=round(optimal_params[1], 2), err=round(3 * sigma_err[1], 2)))
    print(' ')
    ##print(' Covariance matrix:\n', covm)

    return twostep_plot(diameters, mid_points, frequencies, optimal_params, sigma_err)


def two_step(mid_points, frequencies, initial_guess):
    """
    """

    # fit a log normal function
    optimal_params, cov_matrix = curve_fit(log_function, mid_points, frequencies, initial_guess)

    # estimate the uncertainty of the fit.
    sigma_error = sqrt([cov_matrix[0, 0], cov_matrix[1, 1]])

    return optimal_params, sigma_error


def confidence_interval(data, confidence=0.95):
    """Estimate the confidence interval using the t-distribution with n-1
    degrees of freedom t(n-1). This is useful when sample size is small
    and the standard deviation cannot be estimated accurately. For large
    datasets, the t-distribution approaches the normal distribution.

    Parameters
    ----------
    data : array-like
        the dataset

    confidence : float between 0 and 1, optional
        the confidence interval, default = 0.95

    Assumptions
    -----------
    the data follows a normal distrubution (when sample size is large)

    Returns
    -------
    None
    """
    degrees_freedom = len(data) - 1
    sample_mean = np.mean(data)
    sd_err = sem(data)  # Standard error of the mean SD / sqrt(n)
    low, high = t.interval(confidence, degrees_freedom, sample_mean, sd_err)
    err = high - sample_mean

    print(' ')
    print('Confidence set at', confidence * 100, '%')
    print('Mean = {mean} ± {err}' .format(mean=round(sample_mean, 2), err=round(err, 2)))
    print('Max / min = {max} / {min}' .format(max=round(high, 2), min=round(low, 2)))
    print('Coefficient of variation = {} %' .format(round(100 * err / sample_mean, 1)))

    return None


def quartz_piezometer(grain_size, piezometer='Stipp_Tullis'):
    """ Apply different quartz piezometric relations to estimate the differential
    stress from 1D apparent grain sizes. The piezometric relations has the
    following form:

    diff_stress = B * grain_size**-m

    where diff_stress is the differential stress in [MPa], B is an experimentally
    derived parameter in [MPa micron**m], grain_size is the aparent grain size
    in [microns], and m is an experimentally derived exponent.

    Parameters
    ----------
    grain_size : positive scalar
        the apparent grain size in microns

    piezometer : string, optional
        the piezometric relation, either:
            | 'Cross' and 'Cross_hr' from Cross et al. (2017)
            | 'Holyoke' and 'Holyoke_BLG' from Holyoke and Kronenberg (2010)
            | 'Shimizu' from Shimizu (2008)
            | 'Stipp_Tullis' and 'Stipp_Tullis_BLG' from Stipp and Tullis (2003)
            | 'Twiss' from Twiss (1977)

    References
    ----------
    | Cross et al. (2017) https://doi.org/10.1002/2017GL073836
    | De Hoff and Rhines (1968) Quantitative Microscopy. Mcgraw-Hill. New York.
    | Holyoke and Kronenberg (2010) https://doi.org/10.1016/j.tecto.2010.08.001
    | Shimizu (2008) https://doi.org/10.1016/j.jsg.2008.03.004
    | Stipp and Tullis (2003)  https://doi.org/10.1029/2003GL018444
    | Twiss (1977) https://www.doi.org/10.1007/BF01637105

    Assumptions
    -----------
    - Independence of temperature (excepting Shimizu piezometer), total strain,
    flow stress, and water content.

    - Recrystallized grains are equidimensional or close to equidimensional when
    using a single section.

    - The piezometer relations of Stipp and Tullis (2003), Holyoke and Kronenberg
    (2010) and Cross et al. (2007) requires entering the grain size as the square
    root mean apparent grain size calculated using equivalent circular diameters
    with no stereological correction.

    - The piezometer relation of Shimizu (2008) requires entering the grain size
    as the logarithmic median apparent grain size calculated using equivalent
    circular diameters with no stereological correction.

    - The piezometer of Twiss (1977) requires entering the logarithmic mean apparent
    grain size calculated from equivalent circular diameters (ECD) with no stereological
    correction. The function will convert this value to mean linear intercept (LI)
    grain size using the De Hoff and Rhines (1968) empirical relation and assuming
    that LI was originally multiplied by 1.5 (correction factor). Then the final
    relation is: LI = (1.5 / sqrt(4/pi)) * ECD

    Returns
    -------
    The differential stress in MPa, a floating point number
    """

    if piezometer == 'Stipp_Tullis':
        B = 669.0
        m = 0.79
        print('Ensure that you entered the apparent grain size as the square root mean!')

    elif piezometer == 'Stipp_Tullis_BLG':
        B = 1264.1
        m = 1.64
        print('Ensure that you entered the apparent grain size as the square root mean!')

    elif piezometer == 'Holyoke':
        B = 490.3
        m = 0.79
        print('Ensure that you entered the apparent grain size as the square root mean!')

    elif piezometer == 'Holyoke_BLG':
        B = 883.9
        m = 1.85
        print('Ensure that you entered the apparent grain size as the square root mean!')

    elif piezometer == 'Cross':
        B = 593.0
        m = 0.71
        print('Ensure that you entered the apparent grain size as the square root mean!')

    elif piezometer == 'Cross_hr':
        B = 450.9
        m = 0.63
        print('Ensure that you entered the apparent grain size as the square root mean!')

    elif piezometer == 'Shimizu':
        B = 352
        m = 0.8
        T = float(input("Shimizu's paleopiezometer requires entering the temperature [in K] during deformation: "))
        print('Ensure that you entered the apparent grain size as the log median!')

        diff_stress = 352 * grain_size**(-m) * exp(698 / T)

        print(' ')
        print('differential stress =', round(diff_stress, 2), 'MPa')
        return None

    elif piezometer == 'Twiss':
        B = 5.5  # this B value is for grain size in mm (Twiss, 1977)
        m = 0.68
        grain_size = grain_size / 1000  # convert from microns to mm
        grain_size = (1.5 / (sqrt(4 / np.pi))) * grain_size  # convert ECD to LI
        print('Ensure that you entered the apparent grain size as the log mean!')

    else:
        raise ValueError('Piezometer name misspelled. Please choose between valid piezometers')

    diff_stress = B * grain_size**-m

    return('differential stress =', diff_stress, 'MPa')


def olivine_piezometer(grain_size, piezometer='Jung_Karato'):
    """ Apply different olivine piezometric relations to estimate the differential
    stress from 1D apparent grain sizes. The piezometric relations has the
    following expression:

    diff_stress = B * grain_size**-m

    where diff_stress is the differential stress in [MPa], B is an experimentally
    derived parameter in [MPa micron**m], grain_size is the aparent grain size
    in [microns], and m is an experimentally derived exponent which is adimensonal.

    Parameters
    ----------
    grain_size : positive scalar
        the apparent grain size in microns

    piezometer : string, optional
        the piezometric relation, either:
            | 'VanderWal_wet' from Van der Wal et al. (1993)
            | 'Jung_Karato' from Jung and Karato (2001)

    References
    ----------
    | Jung and Karato (2001) https://doi.org/10.1016/S0191-8141(01)00005-0
    | Van der Wal et al. (1993) https://doi.org/10.1029/93GL01382

    Assumptions
    -----------
    - Independence of temperature, total strain, flow stress, and water content.

    - The piezometer of Van der Wal (1993) requires entering the linear mean apparent
    grain size in microns calculated from equivalent circular diameters (ECD) with no
    stereological correction. The function will convert automatically this value to
    linear intercept (LI) grain size using the De Hoff and Rhines (1968) correction.
    Since LI was originally multiplied by 1.2 (correction factor), the final relation
    is: LI = (1.2 / sqrt(4/pi)) * ECD

    - The piezometer of Jung and Karato (2001) requires entering the linear mean
    apparent grain size in microns calculated from equivalent circular diameters
    (ECD) with no stereological correction. The function will convert automatically
    this value to linear intercept (LI) grain size using the De Hoff and Rhines
    (1968) empirical equation. Since LI was originally multiplied by 1.5 (correction
    factor), the final relation is: LI = (1.5 / sqrt(4/pi)) * ECD

    Returns
    -------
    The differential stress in MPa, a floating point number
    """

    if piezometer == 'Jung_Karato':
        B = 5461.03
        m = 0.85
        grain_size = (1.5 / (np.sqrt(4 / np.pi))) * grain_size  # convert ECD to LI
        print('Ensure that you entered the apparent grain size as the linear scale mean!')

    elif piezometer == 'VanderWal_wet':
        B = 0.0425  # this B value requires average grain size in m
        m = 0.75
        grain_size = grain_size / 1e6  # convert from microns to m
        grain_size = (1.2 / (np.sqrt(4 / np.pi))) * grain_size  # convert ECD to LI
        print('Ensure that you entered the apparent grain size as the linear scale mean!')

    else:
        raise ValueError('Piezometer name misspelled. Please choose between valid piezometers')

    diff_stress = B * grain_size**-m

    return('differential stress =', round(diff_stress, 2), 'MPa')


def calcite_piezometers(grain_size, piezometer='calcite_Rutter_SGR'):
    """ Apply different piezometric relations to estimate the differential
    stress from 1D apparent grain sizes. The piezometric relations has the
    following expression:

    diff_stress = B * grain_size**-m

    where diff_stress is the differential stress in [MPa], B is an experimentally
    derived parameter in [MPa micron**m], grain_size is the aparent grain size
    in [microns], and m is an experimentally derived exponent which is adimensonal.

    Parameters
    ----------
    grain_size : positive scalar
        the apparent grain size in microns

    piezometer : string, optional
        the piezometric relation, either:
            | 'calcite_Rutter_SGR' from Rutter (1995)
            | 'calcite_Rutter_GBM' from Rutter (1995)
            | 'albite_PostT_BLG' from Alice and Post (1999)
            | 'calcite_Barnhoorn' from Barnhoorn et al. ()


    References
    ----------
    | Alice and Post (1999) https://doi.org/10.1016/S0040-1951(98)00260-1
    | Barnhoorn et al. (2004) https://doi.org/10.1016/j.jsg.2003.11.024
    | Rutter (1995) https://doi.org/10.1029/95JB02500

    Assumptions
    -----------
    - Independence of temperature, total strain, flow stress, and water content.

    - Recrystallized grains are equidimensional or close to equidimensional when
    using a single section.

    - The piezometer relation of Rutter (1995) requires entering the grain size
    as the root square mean apparent grain size calculated using equivalent
    circular diameters with no stereological correction.

    - The piezometer of Post and Tullis (1999) requires entering the median
    apparent grain size calculated from equivalent circular diameters (ECD) with
    no stereological correction. The function will convert this value to the mean
    linear intercept (LI) grain size using the De Hoff and Rhines (1968) empirical
    relation LI = ECD / sqrt(4/pi)

    Returns
    -------
    The differential stress in MPa, a floating point number
    """

    if piezometer == 'calcite_Rutter_SGR':
        B = 812.83
        m = 0.88
        print('Ensure that you entered the apparent grain size as the square root mean!')

    elif piezometer == 'calcite_Rutter_GBM':
        B = 2691.53
        m = 0.89
        print('Ensure that you entered the apparent grain size as the square root mean!')

    elif piezometer == 'calcite_Barnhoorn':
        B = 537.03
        m = 0.82
        print('Ensure that you entered the apparent grain size as the square root mean!')

    elif piezometer == 'albite_PostT_BLG':
        B = 433.4
        m = 1.52
        grain_size = grain_size / np.sqrt(4 / np.pi)  # convert ECD to LI
        print('Ensure that you entered the apparent grain size as the linear scale median!')

    else:
        raise ValueError('Piezometer name misspelled. Please choose between valid piezometers')

    diff_stress = B * grain_size**-m

    print(' ')
    print('differential stress =', round(diff_stress, 2), 'MPa')
    return None


# ============================================================================ #
# Functions used to generate the plots using the matplotlib library.           #
# It uses hex color codes to set colors.                                       #
# ============================================================================ #


def freq_plot(diameters, binList, xgrid, y_values, y_max, x_peak, mean_GS, median_GS, plot):
    """ Generate a frequency vs grain size plot"""

    fig, ax = plt.subplots()

    ax.hist(diameters,
            bins=binList,
            range=(0, diameters.max()),
            density=True,
            color='#4C72B0',
            edgecolor='#F7FFFF',
            alpha=0.6)
    ax.plot([mean_GS, mean_GS], [0.0001, y_max],
            linestyle='-',
            color='#1F1F1F',
            label='mean',
            linewidth=2)
    ax.plot([median_GS, median_GS], [0.0001, y_max],
            linestyle='--',
            color='#1F1F1F',
            label='median',
            linewidth=2)
    ax.plot(xgrid, y_values,
            color='#2E5A95',
            linewidth=2)

    ax.set_ylabel('frequency',
                  fontsize=13)

    if plot == 'linear':
        ax.set_xlabel(r'apparent diameter ($\mu m$)',
                      fontsize=13)

    elif plot == 'log':
        ax.set_xlabel(r'apparent diameter $\log_e{(\mu m)}$',
                      fontsize=13)

    elif plot == 'sqrt':
        ax.set_xlabel(r'Square root apparent diameter ($\sqrt{\mu m}$)',
                      fontsize=13)

    ax.plot([x_peak], [y_max],
            'o',
            color='#2E5A95',
            label='kde peak')
    ax.vlines(x_peak, 0.0001, y_max,
              linestyle=':',
              color='#2E5A95',
              linewidth=2)
    ax.annotate('Gaussian KDE peak',
                xy=(x_peak, y_max),
                xytext=(+10, +30),
                label='peak')
    ax.legend(loc='upper right',
              fontsize=11)

    fig.tight_layout()

    return plt.show()


def area_weighted_plot(intValues, cumulativeAreas, h, weightedMean):
    """ Generate the area-weighted frequency vs grain size plot"""

    # normalize the y-axis values to percentage of the total area
    totalArea = sum(cumulativeAreas)
    cumulativeAreasNorm = [(x / float(totalArea)) * 100 for x in cumulativeAreas]
    maxValue = max(cumulativeAreasNorm)

    fig, ax = plt.subplots()

    # figure aesthetics
    ax.bar(intValues, cumulativeAreasNorm, width=h,
           color='#55A868',
           edgecolor='#FEFFFF',
           align='edge')
    ax.plot([weightedMean, weightedMean], [0.0001, maxValue],
            linestyle='--',
            color='#1F1F1F',
            label='area weighted mean',
            linewidth=2)
    ax.set_ylabel('% of area fraction',
                  fontsize=13)
    ax.set_xlabel(r'apparent diameter ($\mu m$)',
                  fontsize=13)
    ax.legend(loc='upper right',
              fontsize=11)

    fig.tight_layout()

    return plt.show()


def Saltykov_plot(left_edges, freq3D, binsize, mid_points, cdf_norm):
    """ Generate two plots once the Saltykov method is applied:

    i)  a bar plot (ax1)
    ii) a volume-weighted cumulative frequency plot (ax2)
    """

    fig, (ax1, ax2) = plt.subplots(nrows=1, ncols=2, figsize=(13, 5))

    # frequency plot
    ax1.bar(left_edges, freq3D, width=binsize,
            color='#4C72B0',
            edgecolor='#F7FFFF',
            align='edge')
    ax1.set_ylabel('frequency',
                   fontsize=13)
    ax1.set_xlabel(r'diameter ($\mu m$)',
                   fontsize=13)
    ax1.set_title('estimated 3D grain size distribution',
                  color='#1F1F1F',
                  fontsize=13.5,
                  y=1.02)

    # volume-weighted cumulative frequency curve
    ax2.set_ylim([-2, 105])
    ax2.plot(mid_points, cdf_norm,
             'o-',
             color='#ed4256',
             label='volume weighted CFD',
             linewidth=2)
    ax2.set_ylabel('cumulative volume (%)',
                   fontsize=13)
    ax2.set_xlabel(r'diameter ($\mu m$)',
                   fontsize=13)
    ax2.set_title('volume-weighted cumulative freq. distribution',
                  color='#1F1F1F',
                  fontsize=13.5,
                  y=1.02)

    fig.tight_layout()

    return plt.show()


def twostep_plot(diameters, mid_points, frequencies, optimal_params, sigma_err):
    """ Generate a plot with the best fitting lognormal distribution (two-step method)"""

    # Generate a mesh of x-values
    xgrid = gen_xgrid(diameters, 0.1, max(diameters))

    # Calculate the curve of the best fit
    best_fit = log_function(xgrid, optimal_params[0], optimal_params[1])

    # Estimate all the combinatorial posibilities for fit curves taking into account the uncertainties
    values = array([log_function(xgrid, optimal_params[0] + sigma_err[0], optimal_params[1] + sigma_err[1]),
                    log_function(xgrid, optimal_params[0] - sigma_err[0], optimal_params[1] - sigma_err[1]),
                    log_function(xgrid, optimal_params[0] + sigma_err[0], optimal_params[1] - sigma_err[1]),
                    log_function(xgrid, optimal_params[0] - sigma_err[0], optimal_params[1] + sigma_err[1])])

    # Estimate the standard deviation of the all values obtained
    fit_error = std(values, axis=0)

    # matplotlib stuff
    fig, ax = plt.subplots()

    # bar plot from Saltykov method
    ax.bar(mid_points, frequencies,
           width=mid_points[1] - mid_points[0],
           edgecolor='#1F1F1F',
           hatch='//',
           color='#fff2ae',
           fill=False,
           linewidth=1,
           label='Saltykov method',
           alpha=0.65)

    # plot log-normal distribution
    ax.plot(xgrid, best_fit,
            color='#1F1F1F',
            label='best fit',
            linewidth=2)

    ax.fill_between(xgrid, best_fit + (3 * fit_error), best_fit - (3 * fit_error),
                    color='#525252',
                    label='trust region',
                    alpha=0.5)

    ax.plot(mid_points, frequencies,  # datapoints used for the fitting procedure
            'o',
            color='#d53e4f',
            label='datapoints',
            linewidth=1.5)

    ax.set_ylabel('freq. (per unit vol.)',
                  fontsize=13)
    ax.legend(loc='upper right',
              fontsize=11)
    ax.set_xlabel(r'diameter ($\mu m$)',
                  fontsize=13)

    fig.tight_layout()

    return plt.show()


# ============================================================================== #
# Auxiliary functions used by other functions and doing one task.                #
# The names of the functions are self-explanatory.                               #
# ============================================================================== #

def calc_freq_grainsize(diameters, binsize, plot):
    """ Calculate the distribution of grain sizes using the histogram and Gaussian
    kernel density estimator (KDE). It returns the modal interval, the middle value
    of modal interval, and the frequency peak based on the KDE, and call the
    function responsible for generating the corresponding plot.

    Parameters
    ----------
    diameters : array_like
        the diameters of the grains

    binsize : string (rule of thumb), or posive scalar
        the bin size

    plot : string
        the type of plot and grain size, either 'linear', 'log' or 'sqrt'.

    Call function
    -------------
    - freq_plot
    """

    if len(diameters) < 433:
        print(' ')
        print('Caution! You should use more than 433 grain measurements for reliable results')

    mean_GS, std_GS = mean(diameters), std(diameters)
    RMSmean_GS = sqrt(mean(diameters**2))
    median_GS, iqr_GS = median(diameters), iqr(diameters)

    # estimate the number of classes using an automatic plug-in method (if apply)
    if type(binsize) is str:
        bin_method = binsize
        if plot == 'log':
            histogram, bin_edges = np.histogram(diameters, bins=binsize, range=(1.0, diameters.max()))
        else:
            histogram, bin_edges = np.histogram(diameters, bins=binsize, range=(0.0, diameters.max()))
        binsize = bin_edges[1]
    else:
        bin_method = None
        if plot == 'log':
            bin_edges = np.arange(1.0, diameters.max() + binsize, binsize)
        else:
            bin_edges = np.arange(0, diameters.max() + binsize, binsize)
        histogram, bin_edges = np.histogram(diameters, bins=bin_edges)

    # find the grain size range in which the histogram value is the maximum
    modInt_leftEdge = bin_edges[np.argmax(histogram)]
    modInt_rightEdge = modInt_leftEdge + binsize

    # calculate the Gaussian kernel density function
    # the bandwidth selection is based on the Silverman rule (Silverman 1986)
    kde = gaussian_kde(diameters, bw_method=my_kde_bandwidth)

    # determine where the Gaussian kde function reach it maximum value
    xgrid = gen_xgrid(diameters, 0, diameters.max() + binsize)
    y_values = kde(xgrid)
    y_max, index = np.max(y_values), np.argmax(y_values)
    x_peak = xgrid[index]

    print(' ')
    print('NUMBER WEIGHTED APPROACH with {} apparent grain size:' .format(type))
    print(' ')
    print('Mean grain size = {} microns' .format(round(mean_GS, 2)))
    print('Standard deviation = {} (1-sigma)' .format(round(std_GS, 2)))
    print('RMS mean = {} microns' .format(round(RMSmean_GS, 2)))
    print(' ')
    print('Median grain size = {} microns' .format(round(median_GS, 2)))
    print('Interquartile range (IQR) = {}' .format(round(iqr_GS, 2)))
    print(' ')
    print('HISTOGRAM FEATURES')
    print('The modal interval is {left} - {right}' .format(left=round(modInt_leftEdge, 2), right=round(modInt_rightEdge, 2)))
    print('The number of classes are {}' .format(len(histogram)))
    if type(bin_method) is str:
        print('The bin size is {bin} according to the {rule} rule' .format(bin=round(binsize, 2), rule=bin_method))
    print(' ')
    print('GAUSSIAN KERNEL DENSITY ESTIMATOR FEATURES')
    print('KDE peak (peak grain size) = {} microns' .format(round(x_peak, 2)))
    print('Bandwidth = {} (Silverman rule)' .format(round(kde.covariance_factor() * diameters.std(ddof=1), 2)))
    print(' ')

    return freq_plot(diameters, bin_edges, xgrid, y_values, y_max, x_peak, mean_GS, median_GS, plot)


def gen_xgrid(pop, start, stop):
    """ Returns a mesh of values.

    Parameters
    ----------
    pop : array_like
        the population
    start : ositive scalar
        the starting value of the sequence
    stop : positive scalar
        the end value of the sequence
    """

    d_range = pop.max() - pop.min()

    if d_range < 400:
        density = 2**12
    else:
        density = 2**14

    return np.linspace(start, stop, density)


def my_kde_bandwidth(obj, fac=1.0):
    """ Returns the Silverman bandwidth multiplied by a constant factor
    if neccesary.

    Parameters
    ----------
    obj : Python object
        the kde object

    fac : positive scalar
        the constant factor, 1.0 as default

    Returns
    -------
    The bandwidth of the Gaussian kde. A float.
    """

    bandwidth = np.power(obj.n * (obj.d + 2.0) / 4.0, -1. / (obj.d + 4)) * fac

    return bandwidth


def calc_areaweighted_grainsize(areas, diameters, binsize):
    """ Calculates the area percentage of each grain size interval. It is
    based on Herwegh (2000) and Berger et al. (2011) approach. Returns the
    the grain size interval with the maximum area accumulated, the middle
    value of this interval and the area weighted arithmetic mean.

    References
    ----------
    | Herwegh (2000) doi:10.1016/S0191-8141(99)00165-0
    | Berger et al. (2011) doi:10.1016/j.jsg.2011.07.002

    Parameters
    ----------
    areas: array_like
        a list with the sectional areas of the grains

    diameters: array_like
        a list with the equivalent circular diameters of the grains

    binsize: a string (plug-in methods) or scalar
        the bin size
    """

    # calculate the area weighted arithmetic mean
    areatotal = float(sum(areas))
    weightedAreas = areas / areatotal
    weigtedDiameters = diameters * weightedAreas
    weightedMean = sum(weigtedDiameters)

    # estimate the bin size using an automatic plug-in method (if apply)
    if type(binsize) is str:
        histogram, bin_edges = np.histogram(diameters, bins=binsize, range=(0.0, diameters.max()))
        h = bin_edges[1]
    else:
        bin_edges = np.arange(0.0, diameters.max() + binsize, binsize)
        h = binsize

    cumulativeAreas = []  # Preallocate variable TODO

    # estimate the cumulative areas of each grain size interval TODO -> use enumerate!
    for values in bin_edges:
        mask = np.logical_and(diameters >= values, diameters < (values + h))
        area_sum = np.sum(areas[mask])
        cumulativeAreas.append(round(area_sum, 1))

    # get the index of the maximum value (the modal interval)
    getIndex = cumulativeAreas.index(max(cumulativeAreas))
    print(' ')
    print('AREA WEIGHTED APPROACH:')
    print(' ')
    print('Area-weighted mean grain size = {} microns' .format(round(weightedMean, 2)))
    print(' ')
    print('HISTOGRAM FEATURES')
    print('The modal interval is {left} - {right} microns' .format(left=round(bin_edges[getIndex], 2), right=round(bin_edges[getIndex] + h, 2)))
    print('Midpoint (of modal interval) = {} microns' .format(round((bin_edges[getIndex] + (bin_edges[getIndex] + h)) / 2.0, 1)))
    print('The number of classes are', len(cumulativeAreas) - 1)
    if type(binsize) is str:
        print('The bin size is {bin} according to the {rule} rule' .format(bin=round(h, 2), rule=binsize))
    print(' ')

    return area_weighted_plot(bin_edges, cumulativeAreas, h, weightedMean)


def wicksell_eq(D, d1, d2):
    """ Estimate the cross-section size probability for a population of spheres
    based on Wicksell (1925) and later used by Scheil (1931), Schwartz (1934)
    and Saltykov (1967) to develop the Scheil-Schwartz-Saltykov method. This is
    the generalization by Sahagian and Proussevitch (1998):

    P(r1 < r < r2) = 1/R * (sqrt(R**2 - r1**2) - sqrt(R**2 - r2**2))

    where R is the sphere radius and r the cross-section radius.
    r1 and r2 are the lower and upper bounds of the classes, respectively.

    References
    ----------
    | Sahagian and Proussevitch (1998) doi:10.1029/95JB02500
    | Saltykov (1967) doi:10.1007/978-3-642-88260-9_31
    | Scheil (1931) doi:10.1002/zaac.19312010123
    | Schwartz (1934) Met. Alloy 5:139
    | Wicksell (1925) doi:10.2307/2332027
    | Higgins (2000) doi:10.2138/am-2000-8-901

    Parameters
    ----------
    D: positive scalar
        the midpoint of the actual class, which corresponds with the diameter

    d1: positive scalar
        the lower limit of the bin/class

    d2: positive scalar
        the upper limit of the bin/class
    """

    # convert diameters to radii
    R = D / 2.0
    r1 = d1 / 2.0
    r2 = d2 / 2.0

    return 1 / R * (sqrt(R**2 - r1**2) - sqrt(R**2 - r2**2))


def unfold_population(freq, bin_edges, binsize, mid_points, normalize=True):
    """ Applies the Scheil-Schwartz-Saltykov method to unfold the population of
    apparent (2D) diameters into the actual (3D) population of grain sizes.
    Following the reasoning of Higgins (2000), R (or D) is placed at the center
    of the classes (i.e. the midpoints).

    Reference
    ----------
    Higgins (2000) doi:10.2138/am-2000-8-901

    Parameters
    ----------
    freq: array_like
        a list with the frequency of the different classes

    bin_edges: array_like
        a list with the edges of the classes

    mid_points: array_like
        a list with the midpoints of the classes

    normalize: boolean, optional
        when True negative values of frequency are set to zero and then
        the distribution normalized. It is True by default.

    Returns
    -------
    The normalized frequencies of the unfolded population such that the integral
    over the range is one
    """

    d_values = np.copy(bin_edges)
    midpoints = np.copy(mid_points)
    i = len(midpoints) - 1

    while i > 0:
        j = i
        D = d_values[-1]
        Pi = wicksell_eq(D, d_values[i], d_values[i + 1])

        if freq[i] > 0:
            while j > 0:
                D = midpoints[-1]
                Pj = wicksell_eq(D, d_values[j - 1], d_values[j])
                P_norm = (Pj * freq[i]) / Pi
                np.put(freq, j - 1, freq[j - 1] - P_norm)  # replace specified elements of an array
                j -= 1

            i -= 1
            d_values = delete(d_values, -1)
            midpoints = delete(midpoints, -1)

        # if the value of the current class is zero or negative move to the next class
        else:
            i -= 1
            d_values = delete(d_values, -1)
            midpoints = delete(midpoints, -1)

    if normalize is True:
        freq = np.clip(freq, 0., 2**20)  # replacing negative values with zero TODO
        freq_norm = freq / sum(freq)  # normalize to one
        freq_norm = freq_norm / binsize  # normalize such that the integral over the range is one
        return freq_norm

    else:
        return freq


def log_function(x, shape, scale):
    """ Defines a custom function to fit the data using the scipy curve_fit routine.
    In this case, it is the two-parameter equation that describes a lognormal
    distribution using the mean and the standard deviation of the log(x)
    with base e.

    Parameters
    ----------
    x: array_like
        the x-values

    shape: positive scalar
        the shape parameter; it relates to the sigma parameter: s = log(shape)

    scale: positive scalar
        the scale parameter; it relates to the mean of log(x): m = log(scale)
    """

    s = log(shape)
    m = log(scale)

    return 1 / (x * s * sqrt(2 * np.pi)) * exp(-1 / 2. * ((log(x) - m)**2 / s**2))

# ============================================================================ #


welcome = """
======================================================================================
Welcome to GrainSizeTools script v2.0
======================================================================================
GrainSizeTools is a free open-source cross-platform script to visualize and characterize
the grain size in polycrystalline materials from thin sections and estimate differential
stresses via paleopizometers.
"""
functions_list = """
METHODS AVAILABLE
==================  ==================================================================
List of functions   Description
==================  ==================================================================
extract_areas       Extract the areas of the grains from a text file (txt, csv or xlsx)
calc_diameters      Calculate the diameter via the equivalent circular diameter
find_grain_size     Estimate the apparent grain size and visualize their distribution
Saltykov            Estimate the actual grain size distribution via the Saltykov method
3D_shape            Characterize the shape of the actual grain size distribution
quartz_piezometer   Estimate diff. stress from grain size using quartz piezometers
olivine_piezometer  Estimate diff. stress from grain size using olivine piezometers
calcite_pizometers  Estimate diff. stress from grain size using calcite piezometers
other_pizometers    Estimate diff. stress from grain size using other phases
confidence_interval Estimate the confidence interval according to the t-distribution
==================  ==================================================================

You can get more information about the methods in the following ways:
    (1) Typing help(name of the function in the console. e.g. >>> help(derive3D)
    (2) In the Spyder IDE by writing the name of the function and clicking Ctrl + I
    (3) Visit script documentation at https://marcoalopez.github.io/GrainSizeTools/
"""

print(welcome)
print(functions_list)

if float(np.__version__[0:4]) < 1.11:
    print('The installed Numpy version', np.__version__, 'is too old.')
    print('Please upgrade to v1.11 or higher')

# ============================================================================ #
# Make it correct, make it clear, make it concise, make it fast. In that order.#
#                                                                     Wes Dyer #
# ============================================================================ #
